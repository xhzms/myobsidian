#!/usr/bin/env python3
"""
Brand Asset Crawler
ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë¸Œëœë“œ ìì‚°(ì´ë¯¸ì§€, ìƒ‰ìƒ, í°íŠ¸ ë“±)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
"""

import argparse
import json
import os
import re
import time
from urllib.parse import urljoin, urlparse
from pathlib import Path
from collections import Counter
from dataclasses import dataclass, asdict
from typing import Optional
import hashlib

import requests
from bs4 import BeautifulSoup

# ============================================================
# ì„¤ì •
# ============================================================

DEFAULT_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
}

IMAGE_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg', '.ico'}
SKIP_PATTERNS = [
    'tracking', 'pixel', 'analytics', 'ad-', 'ads-', '1x1',
    'facebook.com/tr', 'google-analytics', 'googletagmanager',
    'doubleclick', 'adservice', 'beacon', 'spacer', 'blank.gif'
]

# ============================================================
# ë°ì´í„° í´ë˜ìŠ¤
# ============================================================

@dataclass
class ImageAsset:
    url: str
    local_path: str
    filename: str
    alt: str
    width: Optional[int]
    height: Optional[int]
    source_page: str
    context: str  # hero, content, footer ë“± í˜ì´ì§€ ë‚´ ìœ„ì¹˜
    css_class: str
    file_size: int = 0

@dataclass
class PageData:
    url: str
    title: str
    images: list
    colors: list
    fonts: list
    texts: dict = None  # í…ìŠ¤íŠ¸ ë°ì´í„° ì¶”ê°€

# ============================================================
# í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤
# ============================================================

class BrandCrawler:
    def __init__(self, base_url: str, output_dir: str, depth: int = 2, delay: float = 1.0):
        self.base_url = base_url
        self.base_domain = urlparse(base_url).netloc
        self.output_dir = Path(output_dir)
        self.depth = depth
        self.delay = delay
        
        self.visited_urls = set()
        self.downloaded_images = set()  # ì¤‘ë³µ ë‹¤ìš´ë¡œë“œ ë°©ì§€
        self.images: list[ImageAsset] = []
        self.pages: list[PageData] = []
        self.colors = Counter()  # ë¹ˆë„ìˆ˜ ê¸°ë°˜ ìƒ‰ìƒ ì¹´ìš´í„°
        self.fonts = set()
        self.texts = {  # í…ìŠ¤íŠ¸ ë°ì´í„°
            'by_page': {},        # í˜ì´ì§€ë³„ ê·¸ë£¹í•‘
            'common': {           # ê³µí†µ ìš”ì†Œ (ë°˜ë³µ ë°œê²¬ ì‹œ)
                'footer': [],
                'navigation': [],
            },
            'unique': {           # ê³ ìœ  ì½˜í…ì¸  (ì¤‘ë³µ ì œê±°)
                'headlines': [],
                'cta_buttons': [],
                'hero_texts': [],
                'meta': [],
            }
        }
        
        self._setup_directories()
    
    def _setup_directories(self):
        """ì¶œë ¥ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±"""
        dirs = [
            self.output_dir / 'raw' / 'images',
            self.output_dir / 'classified',
            self.output_dir / 'data',
        ]
        for d in dirs:
            d.mkdir(parents=True, exist_ok=True)
    
    def _is_same_domain(self, url: str) -> bool:
        """ë™ì¼ ë„ë©”ì¸ í™•ì¸"""
        return urlparse(url).netloc == self.base_domain
    
    def _should_skip_image(self, url: str, width: int = None, height: int = None) -> bool:
        """ìŠ¤í‚µí•  ì´ë¯¸ì§€ì¸ì§€ í™•ì¸ (íŠ¸ë˜í‚¹ í”½ì…€ ë“±)"""
        url_lower = url.lower()
        
        # URL íŒ¨í„´ ì²´í¬
        if any(pattern in url_lower for pattern in SKIP_PATTERNS):
            return True
        
        # 1x1 í”½ì…€ ì²´í¬ (íŠ¸ë˜í‚¹ í”½ì…€)
        if width is not None and height is not None:
            if width <= 2 and height <= 2:
                return True
        
        return False
    
    def _get_image_context(self, img_tag, soup) -> str:
        """ì´ë¯¸ì§€ì˜ í˜ì´ì§€ ë‚´ ìœ„ì¹˜/ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        # ë¶€ëª¨ ìš”ì†Œë“¤ í™•ì¸
        parent = img_tag.parent
        for _ in range(5):  # ìµœëŒ€ 5ë‹¨ê³„ ìƒìœ„ê¹Œì§€
            if parent is None:
                break
            
            parent_class = parent.get('class', [])
            parent_id = parent.get('id', '')
            
            # ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ ë§¤ì¹­
            context_keywords = {
                'hero': ['hero', 'banner', 'slider', 'carousel', 'main-visual'],
                'header': ['header', 'nav', 'logo'],
                'footer': ['footer'],
                'product': ['product', 'item', 'card', 'gallery'],
                'content': ['content', 'article', 'post', 'blog'],
                'sidebar': ['sidebar', 'aside', 'widget'],
                'testimonial': ['testimonial', 'review', 'quote'],
                'team': ['team', 'member', 'staff', 'about'],
            }
            
            all_attrs = ' '.join(parent_class) + ' ' + parent_id
            all_attrs = all_attrs.lower()
            
            for context, keywords in context_keywords.items():
                if any(kw in all_attrs for kw in keywords):
                    return context
            
            parent = parent.parent
        
        return 'unknown'
    
    def _fetch_external_css(self, soup, base_url: str) -> str:
        """ì™¸ë¶€ CSS íŒŒì¼ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
        css_contents = []
        
        for link in soup.find_all('link', rel='stylesheet'):
            href = link.get('href')
            if not href:
                continue
            
            css_url = urljoin(base_url, href)
            
            # ì™¸ë¶€ CDNì€ ìŠ¤í‚µ (Google Fonts ë“±)
            if 'fonts.googleapis.com' in css_url:
                continue
            
            try:
                response = requests.get(css_url, headers=DEFAULT_HEADERS, timeout=5)
                if response.status_code == 200:
                    css_contents.append(response.text)
                    print(f"  ğŸ“„ CSS ë¡œë“œ: {css_url[:60]}...")
            except:
                pass
        
        return '\n'.join(css_contents)
    
    def _extract_colors(self, soup, css_text: str = '', base_url: str = '') -> Counter:
        """CSSì—ì„œ ìƒ‰ìƒ ì¶”ì¶œ (ë¹ˆë„ìˆ˜ í¬í•¨)"""
        colors = []
        
        # ì™¸ë¶€ CSS íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
        external_css = self._fetch_external_css(soup, base_url) if base_url else ''
        
        # ì¸ë¼ì¸ ìŠ¤íƒ€ì¼ì—ì„œ ìƒ‰ìƒ ì¶”ì¶œ
        style_tags = soup.find_all('style')
        all_css = css_text + '\n'.join(tag.string or '' for tag in style_tags) + '\n' + external_css
        
        # ì¸ë¼ì¸ style ì†ì„±ì—ì„œë„ ì¶”ì¶œ
        for elem in soup.find_all(style=True):
            all_css += '\n' + elem.get('style', '')
        
        # HEX ìƒ‰ìƒ (3ìë¦¬, 6ìë¦¬)
        hex_pattern = r'#(?:[0-9a-fA-F]{3}){1,2}\b'
        colors.extend(re.findall(hex_pattern, all_css))
        
        # RGB/RGBA
        rgb_pattern = r'rgba?\s*\([^)]+\)'
        colors.extend(re.findall(rgb_pattern, all_css))
        
        # HSL/HSLA
        hsl_pattern = r'hsla?\s*\([^)]+\)'
        colors.extend(re.findall(hsl_pattern, all_css))
        
        # ì •ê·œí™” + í•„í„°ë§
        normalized = []
        skip_colors = {'#fff', '#ffffff', '#000', '#000000'}
        
        for color in colors:
            # ì†Œë¬¸ìë¡œ í†µì¼, ê³µë°± ì œê±°
            c = color.lower().replace(' ', '')
            
            # í‘ë°±/íˆ¬ëª… ì œì™¸
            if c in skip_colors or 'transparent' in c or 'rgba(0,0,0,0)' in c:
                continue
            
            # HEX 3ìë¦¬ â†’ 6ìë¦¬ë¡œ í™•ì¥
            if c.startswith('#') and len(c) == 4:
                c = f'#{c[1]*2}{c[2]*2}{c[3]*2}'
            
            normalized.append(c)
        
        return Counter(normalized)
    
    def _extract_fonts(self, soup, css_text: str = '') -> set:
        """CSSì—ì„œ í°íŠ¸ ì¶”ì¶œ"""
        fonts = set()
        
        style_tags = soup.find_all('style')
        all_css = css_text + '\n'.join(tag.string or '' for tag in style_tags)
        
        # font-family ì¶”ì¶œ
        font_pattern = r'font-family:\s*([^;]+)'
        matches = re.findall(font_pattern, all_css)
        
        for match in matches:
            # ê°œë³„ í°íŠ¸ ë¶„ë¦¬
            font_list = [f.strip().strip('"\'') for f in match.split(',')]
            fonts.update(font_list)
        
        # Google Fonts ë§í¬ ì¶”ì¶œ
        link_tags = soup.find_all('link', href=re.compile(r'fonts\.googleapis\.com'))
        for link in link_tags:
            href = link.get('href', '')
            # family íŒŒë¼ë¯¸í„° ì¶”ì¶œ
            family_match = re.search(r'family=([^&]+)', href)
            if family_match:
                families = family_match.group(1).replace('+', ' ').split('|')
                fonts.update(f.split(':')[0] for f in families)
        
        return fonts
    
    def _process_texts(self) -> dict:
        """í…ìŠ¤íŠ¸ í›„ì²˜ë¦¬: ê³µí†µ ìš”ì†Œ ë¶„ë¦¬ + ê³ ìœ  ì½˜í…ì¸  ì¶”ì¶œ"""
        from collections import Counter
        
        # ëª¨ë“  í…ìŠ¤íŠ¸ ìˆ˜ì§‘
        all_headlines = []
        all_cta = []
        all_nav = []
        all_hero = []
        all_meta = []
        
        for url, page_texts in self.texts['by_page'].items():
            for h in page_texts.get('headlines', []):
                all_headlines.append(h['text'])
            for c in page_texts.get('cta_buttons', []):
                all_cta.append(c['text'])
            all_nav.extend(page_texts.get('navigation', []))
            for h in page_texts.get('hero_texts', []):
                all_hero.append(h['text'])
            for m in page_texts.get('meta', []):
                all_meta.append(m['text'])
        
        # ë¹ˆë„ìˆ˜ ê³„ì‚°
        headline_counts = Counter(all_headlines)
        cta_counts = Counter(all_cta)
        nav_counts = Counter(all_nav)
        
        total_pages = len(self.texts['by_page'])
        threshold = max(2, total_pages * 0.5)  # 50% ì´ìƒ í˜ì´ì§€ì—ì„œ ë°˜ë³µë˜ë©´ ê³µí†µ ìš”ì†Œ
        
        # ê³µí†µ ìš”ì†Œ ë¶„ë¦¬ (footer, navigation)
        common_headlines = {text for text, count in headline_counts.items() if count >= threshold}
        common_cta = {text for text, count in cta_counts.items() if count >= threshold}
        common_nav = {text for text, count in nav_counts.items() if count >= threshold}
        
        # ê³ ìœ  ì½˜í…ì¸  ì¶”ì¶œ (ì¤‘ë³µ ì œê±°)
        unique_headlines = []
        unique_cta = []
        unique_hero = []
        unique_meta = []
        
        seen_headlines = set()
        seen_cta = set()
        seen_hero = set()
        seen_meta = set()
        
        for url, page_texts in self.texts['by_page'].items():
            for h in page_texts.get('headlines', []):
                text = h['text']
                if text not in common_headlines and text not in seen_headlines:
                    seen_headlines.add(text)
                    unique_headlines.append({
                        'tag': h['tag'],
                        'text': text,
                        'page': url
                    })
            
            for c in page_texts.get('cta_buttons', []):
                text = c['text']
                if text not in common_cta and text not in seen_cta:
                    seen_cta.add(text)
                    unique_cta.append({
                        'text': text,
                        'page': url
                    })
            
            for h in page_texts.get('hero_texts', []):
                text = h['text']
                if text not in seen_hero:
                    seen_hero.add(text)
                    unique_hero.append({
                        'text': text,
                        'page': url
                    })
            
            for m in page_texts.get('meta', []):
                text = m['text']
                if text not in seen_meta:
                    seen_meta.add(text)
                    unique_meta.append(m)
        
        return {
            'common': {
                'footer': sorted(list(common_headlines | common_cta)),
                'navigation': sorted(list(common_nav)),
            },
            'unique': {
                'headlines': unique_headlines,
                'cta_buttons': unique_cta,
                'hero_texts': unique_hero,
                'meta': unique_meta,
            },
            'by_page': self.texts['by_page']  # ì›ë³¸ë„ ìœ ì§€
        }
    
    def _extract_texts(self, soup, url: str) -> dict:
        """í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        texts = {
            'headlines': [],
            'cta_buttons': [],
            'navigation': [],
            'hero_texts': [],
            'meta': [],
        }
        
        def clean_text(text):
            """\ud14d\uc2a4\ud2b8 \uc815\ub9ac"""
            if not text:
                return ''
            return ' '.join(text.strip().split())
        
        # 1. í—¤ë“œë¼ì¸ (h1, h2, h3)
        for tag in ['h1', 'h2', 'h3']:
            for elem in soup.find_all(tag):
                text = clean_text(elem.get_text())
                if text and len(text) > 2:
                    texts['headlines'].append({
                        'tag': tag,
                        'text': text,
                        'page': url
                    })
        
        # 2. CTA ë²„íŠ¼ (button, a.btn, a.button, input[type=submit])
        cta_selectors = [
            'button',
            'a[class*="btn"]',
            'a[class*="button"]',
            'a[class*="cta"]',
            'input[type="submit"]',
            '[class*="cta"]',
        ]
        for selector in cta_selectors:
            for elem in soup.select(selector):
                text = clean_text(elem.get_text()) or elem.get('value', '')
                if text and len(text) > 1 and len(text) < 50:
                    if text not in [item['text'] for item in texts['cta_buttons']]:
                        texts['cta_buttons'].append({
                            'text': text,
                            'page': url
                        })
        
        # 3. ë„¤ë¹„ê²Œì´ì…˜ ë©”ë‰´
        nav_elements = soup.find_all(['nav', 'header'])
        for nav in nav_elements:
            for link in nav.find_all('a'):
                text = clean_text(link.get_text())
                if text and len(text) > 1 and len(text) < 30:
                    if text not in texts['navigation']:
                        texts['navigation'].append(text)
        
        # 4. íˆì–´ë¡œ ì„¹ì…˜ í…ìŠ¤íŠ¸
        hero_selectors = [
            '[class*="hero"]',
            '[class*="banner"]',
            '[class*="main-visual"]',
            '[class*="jumbotron"]',
            '[class*="intro"]',
            'section:first-of-type',
        ]
        for selector in hero_selectors:
            for elem in soup.select(selector):
                # í° í…ìŠ¤íŠ¸ ì°¾ê¸° (p, span ë“±)
                for p in elem.find_all(['p', 'span', 'div']):
                    text = clean_text(p.get_text())
                    if text and 20 < len(text) < 200:  # ì ì ˆí•œ ê¸¸ì´
                        if text not in [item['text'] for item in texts['hero_texts']]:
                            texts['hero_texts'].append({
                                'text': text,
                                'page': url
                            })
        
        # 5. ë©”íƒ€ ë””ìŠ¤í¬ë¦½ì…˜
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            texts['meta'].append({
                'type': 'description',
                'text': clean_text(meta_desc['content']),
                'page': url
            })
        
        og_desc = soup.find('meta', property='og:description')
        if og_desc and og_desc.get('content'):
            texts['meta'].append({
                'type': 'og:description', 
                'text': clean_text(og_desc['content']),
                'page': url
            })
        
        og_title = soup.find('meta', property='og:title')
        if og_title and og_title.get('content'):
            texts['meta'].append({
                'type': 'og:title',
                'text': clean_text(og_title['content']),
                'page': url
            })
        
        return texts
    
    def _download_image(self, img_url: str, source_page: str) -> Optional[str]:
        """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ë¡œì»¬ ê²½ë¡œ ë°˜í™˜"""
        # ì¤‘ë³µ ë‹¤ìš´ë¡œë“œ ë°©ì§€
        if img_url in self.downloaded_images:
            return None
        
        try:
            response = requests.get(img_url, headers=DEFAULT_HEADERS, timeout=10)
            response.raise_for_status()
            
            # íŒŒì¼ëª… ìƒì„± (URL í•´ì‹œ + ì›ë³¸ í™•ì¥ì)
            parsed = urlparse(img_url)
            ext = Path(parsed.path).suffix.lower() or '.jpg'
            if ext not in IMAGE_EXTENSIONS:
                ext = '.jpg'
            
            url_hash = hashlib.md5(img_url.encode()).hexdigest()[:12]
            filename = f"{url_hash}{ext}"
            local_path = self.output_dir / 'raw' / 'images' / filename
            
            with open(local_path, 'wb') as f:
                f.write(response.content)
            
            # ë‹¤ìš´ë¡œë“œ í›„ í¬ê¸° ì²´í¬ (íŠ¸ë˜í‚¹ í”½ì…€ + ë¹ˆ íŒŒì¼ í•„í„°ë§)
            file_size = os.path.getsize(local_path)
            if file_size == 0:
                os.remove(local_path)
                print(f"  â­ï¸ ë¹ˆ íŒŒì¼ ì œì™¸: {filename}")
                return None
            
            # SVG ë‚´ìš© ì²´í¬ (ì‹¤ì œ ê·¸ë˜í”½ ìš”ì†Œê°€ ìˆëŠ”ì§€)
            if ext == '.svg':
                try:
                    with open(local_path, 'r', encoding='utf-8') as f:
                        svg_content = f.read()
                    # path, rect, circle, polygon ë“± ì‹¤ì œ ê·¸ë˜í”½ ìš”ì†Œ í™•ì¸
                    if not re.search(r'<(path|rect|circle|polygon|ellipse|line|polyline|image|text|g)\s', svg_content):
                        os.remove(local_path)
                        print(f"  â­ï¸ ë¹ˆ SVG ì œì™¸: {filename}")
                        return None
                except:
                    pass
            
            try:
                from PIL import Image
                with Image.open(local_path) as img:
                    w, h = img.size
                    if w <= 2 and h <= 2:
                        os.remove(local_path)
                        print(f"  â­ï¸ íŠ¸ë˜í‚¹ í”½ì…€ ì œì™¸: {filename} ({w}x{h})")
                        return None
            except:
                pass  # PILë¡œ ì—´ ìˆ˜ ì—†ëŠ” íŒŒì¼ì€ í†µê³¼
            
            self.downloaded_images.add(img_url)
            return str(local_path)
            
        except Exception as e:
            print(f"  âš ï¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {img_url[:50]}... ({e})")
            return None
    
    def _crawl_page(self, url: str) -> Optional[PageData]:
        """ë‹¨ì¼ í˜ì´ì§€ í¬ë¡¤ë§"""
        if url in self.visited_urls:
            return None
        
        self.visited_urls.add(url)
        print(f"\nğŸ“„ í¬ë¡¤ë§: {url}")
        
        try:
            response = requests.get(url, headers=DEFAULT_HEADERS, timeout=15)
            response.raise_for_status()
        except Exception as e:
            print(f"  âš ï¸ í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        title = soup.title.string if soup.title else ''
        
        # ì´ë¯¸ì§€ ì¶”ì¶œ
        page_images = []
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
            if not src:
                continue
            
            img_url = urljoin(url, src)
            
            if self._should_skip_image(img_url):
                continue
            
            # srcsetì—ì„œ ê°€ì¥ í° ì´ë¯¸ì§€ ì¶”ì¶œ
            srcset = img.get('srcset')
            if srcset:
                srcset_urls = re.findall(r'(https?://[^\s]+)', srcset)
                if srcset_urls:
                    img_url = srcset_urls[-1]  # ë³´í†µ ë§ˆì§€ë§‰ì´ ê°€ì¥ í¼
            
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            local_path = self._download_image(img_url, url)
            if not local_path:
                continue
            
            # ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
            asset = ImageAsset(
                url=img_url,
                local_path=local_path,
                filename=Path(local_path).name,
                alt=img.get('alt', ''),
                width=int(img.get('width', 0)) if img.get('width', '').isdigit() else None,
                height=int(img.get('height', 0)) if img.get('height', '').isdigit() else None,
                source_page=url,
                context=self._get_image_context(img, soup),
                css_class=' '.join(img.get('class', [])),
                file_size=os.path.getsize(local_path)
            )
            
            page_images.append(asset)
            self.images.append(asset)
            print(f"  ğŸ–¼ï¸ {asset.filename} ({asset.context})")
        
        # CSS background-image ì¶”ì¶œ
        for elem in soup.find_all(style=re.compile(r'background.*url')):
            style = elem.get('style', '')
            bg_urls = re.findall(r'url\([\'"]?([^\'")]+)[\'"]?\)', style)
            for bg_url in bg_urls:
                img_url = urljoin(url, bg_url)
                if self._should_skip_image(img_url):
                    continue
                
                local_path = self._download_image(img_url, url)
                if local_path:
                    asset = ImageAsset(
                        url=img_url,
                        local_path=local_path,
                        filename=Path(local_path).name,
                        alt='',
                        width=None,
                        height=None,
                        source_page=url,
                        context='background',
                        css_class=' '.join(elem.get('class', [])),
                        file_size=os.path.getsize(local_path)
                    )
                    page_images.append(asset)
                    self.images.append(asset)
        
        # OG ì´ë¯¸ì§€ ì¶”ì¶œ
        og_image = soup.find('meta', property='og:image')
        if og_image and og_image.get('content'):
            img_url = urljoin(url, og_image['content'])
            local_path = self._download_image(img_url, url)
            if local_path:
                asset = ImageAsset(
                    url=img_url,
                    local_path=local_path,
                    filename=Path(local_path).name,
                    alt='og:image',
                    width=None,
                    height=None,
                    source_page=url,
                    context='social',
                    css_class='',
                    file_size=os.path.getsize(local_path)
                )
                page_images.append(asset)
                self.images.append(asset)
        
        # ìƒ‰ìƒ/í°íŠ¸/í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì™¸ë¶€ CSS í¬í•¨)
        page_colors = self._extract_colors(soup, base_url=url)
        page_fonts = self._extract_fonts(soup)
        page_texts = self._extract_texts(soup, url)
        
        self.colors.update(page_colors)
        self.fonts.update(page_fonts)
        
        # í…ìŠ¤íŠ¸ ëˆ„ì  (í˜ì´ì§€ë³„ ê·¸ë£¹í•‘)
        self.texts['by_page'][url] = page_texts
        
        page_data = PageData(
            url=url,
            title=title,
            images=[asdict(img) for img in page_images],
            colors=list(page_colors),
            fonts=list(page_fonts),
            texts=page_texts
        )
        self.pages.append(page_data)
        
        return page_data
    
    def _get_subpage_links(self, soup, current_url: str) -> list:
        """ì„œë¸Œí˜ì´ì§€ ë§í¬ ì¶”ì¶œ"""
        links = []
        for a in soup.find_all('a', href=True):
            href = a['href']
            full_url = urljoin(current_url, href)
            
            # í•„í„°ë§
            if not self._is_same_domain(full_url):
                continue
            if full_url in self.visited_urls:
                continue
            if '#' in full_url:
                full_url = full_url.split('#')[0]
            if any(ext in full_url.lower() for ext in ['.pdf', '.zip', '.mp4']):
                continue
            
            links.append(full_url)
        
        return list(set(links))
    
    def crawl(self):
        """í¬ë¡¤ë§ ì‹¤í–‰"""
        print(f"\n{'='*60}")
        print(f"ğŸš€ ë¸Œëœë“œ ìì‚° í¬ë¡¤ë§ ì‹œì‘")
        print(f"   URL: {self.base_url}")
        print(f"   ê¹Šì´: {self.depth}")
        print(f"   ì¶œë ¥: {self.output_dir}")
        print(f"{'='*60}")
        
        urls_to_crawl = [(self.base_url, 0)]
        
        while urls_to_crawl:
            url, current_depth = urls_to_crawl.pop(0)
            
            if current_depth > self.depth:
                continue
            
            try:
                response = requests.get(url, headers=DEFAULT_HEADERS, timeout=15)
                soup = BeautifulSoup(response.text, 'html.parser')
            except:
                continue
            
            self._crawl_page(url)
            
            # ì„œë¸Œí˜ì´ì§€ ì¶”ê°€
            if current_depth < self.depth:
                subpages = self._get_subpage_links(soup, url)
                for subpage in subpages[:20]:  # í˜ì´ì§€ë‹¹ ìµœëŒ€ 20ê°œ
                    if subpage not in self.visited_urls:
                        urls_to_crawl.append((subpage, current_depth + 1))
            
            time.sleep(self.delay)
        
        self._save_results()
    
    def _save_results(self):
        """ê²°ê³¼ ì €ì¥"""
        data_dir = self.output_dir / 'data'
        
        # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„°
        with open(data_dir / 'images.json', 'w', encoding='utf-8') as f:
            json.dump([asdict(img) for img in self.images], f, indent=2, ensure_ascii=False)
        
        # í˜ì´ì§€ ë°ì´í„°
        with open(data_dir / 'pages.json', 'w', encoding='utf-8') as f:
            json.dump([asdict(p) for p in self.pages], f, indent=2, ensure_ascii=False)
        
        # ìƒ‰ìƒ (ë¹ˆë„ìˆ˜ ìˆœ ì •ë ¬)
        sorted_colors = [
            {"color": color, "count": count}
            for color, count in self.colors.most_common()
        ]
        with open(data_dir / 'colors.json', 'w', encoding='utf-8') as f:
            json.dump(sorted_colors, f, indent=2, ensure_ascii=False)
        
        # Top 10 ë¸Œëœë“œ ì»¨ëŸ¬ ì¶œë ¥
        print(f"\nğŸ¨ Top 10 ë¸Œëœë“œ ì»¨ëŸ¬:")
        for i, item in enumerate(sorted_colors[:10], 1):
            print(f"   {i:2}. {item['color']:20} ({item['count']}íšŒ)")
        
        # í°íŠ¸
        with open(data_dir / 'fonts.json', 'w', encoding='utf-8') as f:
            json.dump(list(self.fonts), f, indent=2)
        
        # í…ìŠ¤íŠ¸ (ê³µí†µ ìš”ì†Œ ë¶„ë¦¬ + ê³ ìœ  ì½˜í…ì¸  ì¶”ì¶œ)
        processed_texts = self._process_texts()
        with open(data_dir / 'texts.json', 'w', encoding='utf-8') as f:
            json.dump(processed_texts, f, indent=2, ensure_ascii=False)
        
        # í…ìŠ¤íŠ¸ ìš”ì•½ ì¶œë ¥
        print(f"\nğŸ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ê²°ê³¼:")
        print(f"   í˜ì´ì§€: {len(self.texts['by_page'])}ê°œ")
        print(f"   ê³µí†µ ìš”ì†Œ: {len(processed_texts['common']['footer'])}ê°œ")
        print(f"   ê³ ìœ  í—¤ë“œë¼ì¸: {len(processed_texts['unique']['headlines'])}ê°œ")
        print(f"   ê³ ìœ  CTA: {len(processed_texts['unique']['cta_buttons'])}ê°œ")
        
        # ìš”ì•½ ì¶œë ¥
        print(f"\n{'='*60}")
        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"   í˜ì´ì§€: {len(self.pages)}ê°œ")
        print(f"   ì´ë¯¸ì§€: {len(self.images)}ê°œ")
        print(f"   ìƒ‰ìƒ: {len(self.colors)}ê°œ")
        print(f"   í°íŠ¸: {len(self.fonts)}ê°œ")
        print(f"\nğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {self.output_dir}")
        print(f"{'='*60}")

# ============================================================
# ë©”ì¸
# ============================================================

def main():
    parser = argparse.ArgumentParser(description='ë¸Œëœë“œ ìì‚° í¬ë¡¤ëŸ¬')
    parser.add_argument('--url', required=True, help='ì‹œì‘ URL')
    parser.add_argument('--output', default='./output', help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--depth', type=int, default=2, help='í¬ë¡¤ë§ ê¹Šì´')
    parser.add_argument('--delay', type=float, default=1.0, help='ìš”ì²­ ê°„ ë”œë ˆì´(ì´ˆ)')
    
    args = parser.parse_args()
    
    crawler = BrandCrawler(
        base_url=args.url,
        output_dir=args.output,
        depth=args.depth,
        delay=args.delay
    )
    
    crawler.crawl()

if __name__ == '__main__':
    main()
